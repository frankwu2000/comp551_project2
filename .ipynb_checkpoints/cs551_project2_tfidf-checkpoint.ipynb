{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x7f', '\\x80', '\\x92', '\\x9c', 'Â¡', 'Â¢', 'Â£', 'Â¤', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', 'Â¬', '\\xad', 'Â®', 'Â¯', 'Â°', 'Â±', 'Â²', 'Â³', 'Â´', 'Âµ', 'Â¶', 'Â·', 'Â¸', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½', 'Â¾', 'Â¿', 'Ã—', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã·', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¿', 'Ä…', 'Ä‡', 'Ä', 'Ä', 'Ä•', 'Ä—', 'Ä™', 'Ä›', 'ÄŸ', 'Ä±', 'Äº', 'Ä¾', 'Å‚', 'Å„', 'Åˆ', 'Å', 'Å‘', 'Å“', 'Å•', 'Å™', 'Å›', 'Å', 'ÅŸ', 'Å¡', 'Å¥', 'Å«', 'Å¯', 'Å±', 'Åº', 'Å¼', 'Å¾', 'Æ’', 'É', 'É‘', 'Éš', 'É›', 'É´', 'Ê', 'Ê’', 'Ê–', 'Êœ']\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x7f', '\\x80', '\\x92', '\\x9c', 'Â¡', 'Â¢', 'Â£', 'Â¤', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', 'Â¬', '\\xad', 'Â®', 'Â¯', 'Â°', 'Â±', 'Â²', 'Â³', 'Â´', 'Âµ', 'Â¶', 'Â·', 'Â¸', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½', 'Â¾', 'Â¿', 'Ã—', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã·', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¿', 'Ä…', 'Ä‡', 'Ä', 'Ä', 'Ä•', 'Ä—', 'Ä™', 'Ä›', 'ÄŸ', 'Ä±', 'Äº', 'Ä¾', 'Å‚', 'Å„', 'Åˆ', 'Å', 'Å‘', 'Å“', 'Å•', 'Å™', 'Å›', 'Å', 'ÅŸ', 'Å¡', 'Å¥', 'Å«', 'Å¯', 'Å±', 'Åº', 'Å¼', 'Å¾', 'Æ’', 'É', 'É‘', 'Éš', 'É›', 'É´', 'Ê', 'Ê’', 'Ê–', 'Êœ', 'Ë†', 'Ë‡', 'Ëˆ', 'Ë‹', 'Ëœ', 'Ì€', 'Ì', 'Ìƒ', 'Ì‡', 'Ì›', 'Ì§', 'Ì¯', 'Ìµ', 'Ì¶', 'Ì¿', 'Í€', 'Íœ', 'Í', 'Í¡', 'Î±', 'Î´', 'Îµ', 'Î·', 'Î¸', 'Î¹', 'Î»', 'Î¼', 'Î½', 'Î¾', 'Î¿', 'Ï€', 'Ï', 'Ï‚', 'Ïƒ', 'Ï„', 'Ï‰', 'Ï', 'Ğ°', 'Ğ±', 'Ğ³', 'Ğ´', 'Ğµ', 'Ğ»', 'Ğ½', 'Ğ¾', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ…', 'ÑŒ', 'Ñ', 'Ò‰', 'Ô', '×‘', '×“', '×”', '×•', '×¨', '×ª', 'Ø§', 'Ø¨', 'Øª', 'Ø®', 'Ø¯', 'Ø±', 'Ø²', 'Ø³', 'Ø¹', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'ÛŒ', 'à² ', 'à²¥', 'à¼½', 'áƒš', 'á…Œ', 'á´—', 'á´œ', '\\u200b', '\\u200d', 'â€“', 'â€”', 'â€•', 'â€˜', 'â€™', 'â€š', 'â€œ', 'â€', 'â€', 'â€ ', 'â€¡', 'â€¢', 'â€¦', 'â€°', 'â€²', 'â€¹', 'â€º', 'â€¼', 'â€¿', 'â‰', 'â‚¬', 'â„…', 'â„¢', 'â†‘', 'â†’', 'â†—', 'âˆ€', 'âˆš', 'â‰ˆ', 'â‰ ', 'â‰¡', 'â‰¦', 'â‰§', 'âŠ‚', 'âŠ„', 'âŒ', 'âŒ˜', 'âŒš', 'â˜', 'â©', 'â±', 'â“', 'â”€', 'â”Š', 'â”¬', 'â•­', 'â•²', 'â–€', 'â– ', 'â–¶', 'â–·', 'â–¼', 'â—', 'â—', 'â—•', 'â—¡', 'â˜€', 'â˜…', 'â˜’', 'â˜•', 'â˜', 'â˜¹', 'â˜º', 'â™€', 'â™‚', 'â™›', 'â™¡', 'â™¥', 'â™¦', 'â™ª', 'â™«', 'âš†', 'âš ', 'âš¡', 'âšª', 'âš½', 'â›³', 'âœ…', 'âœˆ', 'âœŠ', 'âœŒ', 'âœ“', 'âœ”', 'âœ¥', 'âœ¨', 'âœ¿', 'â¤', 'â¶', 'â', 'â', 'â', 'â', 'â¡', 'â­', 'ã€Œ', 'ã€', 'ã€', 'ã€‘', 'ãˆ', 'ã', 'ã—', 'ã™', 'ã¤', 'ã¾', 'ã¿', 'ãƒ„', 'ãƒ', 'ãƒ¢', 'ãƒ®', 'ãƒ½', 'ãƒ¾', 'ä¹‡', 'å', 'å¤©', 'å§‹', 'å¼€', 'æˆ‘', 'æ˜', 'ç‚¹', 'ç›Š', 'ç§€', '\\uf0d8', '\\uf8ff', 'ï¸', 'ï¹', '\\ufeff', 'ï¼', 'ï¼‡', 'ï¼‰', 'ï¼Œ', 'ï¼Ÿ', 'ï½€', 'ï½', 'ï½ƒ', 'ï½„', 'ï½…', 'ï½‡', 'ï½ˆ', 'ï½‰', 'ï½Œ', 'ï½', 'ï½', 'ï½', 'ï½’', 'ï½“', 'ï½•', 'ï¾‰', 'ï¾Ÿ', 'ï¿½', 'ğ„', 'ğ€', 'ğ„', 'ğ‘', 'ğ™', 'ğ”', 'ğ”', 'ğ”¢', 'ğ”¦', 'ğ”«', 'ğ”¯', 'ğ˜“', 'ğ˜¢', 'ğ˜¤', 'ğ˜¥', 'ğ˜¦', 'ğ˜§', 'ğ˜©', 'ğ˜ª', 'ğ˜­', 'ğ˜³', 'ğ˜´', 'ğ˜µ', 'ğ˜¶', 'ğ˜¹', 'ğŸ…±', 'ğŸ‡¦', 'ğŸ‡§', 'ğŸ‡ª', 'ğŸ‡«', 'ğŸ‡¬', 'ğŸ‡®', 'ğŸ‡·', 'ğŸ‡¸', 'ğŸ‡º', 'ğŸŒ€', 'ğŸŒˆ', 'ğŸŒ', 'ğŸŒœ', 'ğŸŒ', 'ğŸŒŸ', 'ğŸŒ ', 'ğŸŒ³', 'ğŸŒ¶', 'ğŸŒ·', 'ğŸŒ¹', 'ğŸŒ»', 'ğŸŒ¼', 'ğŸŒ¿', 'ğŸƒ', 'ğŸ«', 'ğŸ·', 'ğŸ»', 'ğŸ¿', 'ğŸ', 'ğŸ‚', 'ğŸ‰', 'ğŸŠ', 'ğŸ¤', 'ğŸ¥', 'ğŸ§', 'ğŸ¬', 'ğŸ®', 'ğŸµ', 'ğŸ¶', 'ğŸº', 'ğŸ€', 'ğŸ', 'ğŸƒ', 'ğŸ†', 'ğŸ‹', 'ğŸŒ', 'ğŸ', 'ğŸ', 'ğŸ«', 'ğŸ´', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ', 'ğŸ’', 'ğŸ‘€', 'ğŸ‘…', 'ğŸ‘†', 'ğŸ‘‰', 'ğŸ‘Š', 'ğŸ‘‹', 'ğŸ‘Œ', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘”', 'ğŸ‘¥', 'ğŸ‘¨', 'ğŸ‘©', 'ğŸ‘®', 'ğŸ‘²', 'ğŸ’ƒ', 'ğŸ’…', 'ğŸ’‹', 'ğŸ’', 'ğŸ’“', 'ğŸ’”', 'ğŸ’•', 'ğŸ’–', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ’œ', 'ğŸ’¤', 'ğŸ’¥', 'ğŸ’©', 'ğŸ’ª', 'ğŸ’«', 'ğŸ’¬', 'ğŸ’¯', 'ğŸ’»', 'ğŸ“ˆ', 'ğŸ“', 'ğŸ“', 'ğŸ“š', 'ğŸ“¢', 'ğŸ“²', 'ğŸ“º', 'ğŸ“½', 'ğŸ”', 'ğŸ”¥', 'ğŸ”´', 'ğŸ”µ', 'ğŸ•Š', 'ğŸ–’', 'ğŸ–±', 'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‡', 'ğŸ˜ˆ', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜’', 'ğŸ˜“', 'ğŸ˜”', 'ğŸ˜•', 'ğŸ˜–', 'ğŸ˜˜', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ˜£', 'ğŸ˜¤', 'ğŸ˜¥', 'ğŸ˜§', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜ª', 'ğŸ˜«', 'ğŸ˜¬', 'ğŸ˜­', 'ğŸ˜®', 'ğŸ˜¯', 'ğŸ˜±', 'ğŸ˜²', 'ğŸ˜³', 'ğŸ˜´', 'ğŸ˜µ', 'ğŸ˜¶', 'ğŸ˜·', 'ğŸ˜¹', 'ğŸ˜»', 'ğŸ™', 'ğŸ™‚', 'ğŸ™ƒ', 'ğŸ™„', 'ğŸ™…', 'ğŸ™‡', 'ğŸ™ˆ', 'ğŸ™Š', 'ğŸ™‹', 'ğŸ™Œ', 'ğŸ™', 'ğŸ™', 'ğŸš…', 'ğŸš‹', 'ğŸš‘', 'ğŸš•', 'ğŸš¨', 'ğŸš«', 'ğŸš¶', 'ğŸš½', 'ğŸš¿', 'ğŸ¤’', 'ğŸ¤“', 'ğŸ¤”', 'ğŸ¤—', 'ğŸ¤˜', 'ğŸ¤™', 'ğŸ¤', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤¢', 'ğŸ¤£', 'ğŸ¤¤', 'ğŸ¤¥', 'ğŸ¤¦', '\\U0001f929', '\\U0001f92a', 'ğŸ¤·', 'ğŸ¥€', 'ğŸ¥‡', 'ğŸ¥™', 'ğŸ¦„', 'ğŸ¦Š', 'ğŸ¦‹']\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import csv\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import *\n",
    "\n",
    "train_x_raw = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "test_x_raw = []\n",
    "test_x = []\n",
    "\n",
    "\n",
    "#load two train data files, one test data file, and formatting them\n",
    "def load_dataset():\n",
    "    \n",
    "    global train_x_raw\n",
    "    global train_y\n",
    "    global test_x_raw\n",
    "    \n",
    "    with open(\"data_set/train_set_x.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            text_deleteurl = re.sub(r\"http\\S+\",\"\", row[1])\n",
    "            text_deletenum=re.sub(\"\\d+\",\"\",text_deleteurl)\n",
    "            l=text_deletenum.replace(\" \",\"\").lower()\n",
    "            train_x_raw.append(l)\n",
    "\n",
    "    with open(\"data_set/train_set_y.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            train_y.append(row[1])\n",
    "        \n",
    "    with open(\"data_set/test_set_x.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            text_deletenum=re.sub(\"\\d+\",\"\",row[1])\n",
    "            l=text_deletenum.replace(\" \",\"\").lower()\n",
    "            test_x_raw.append(l)\n",
    "\n",
    "def tfidf_preprocess():\n",
    "#tfidf preprocessing\n",
    "    global train_x_raw\n",
    "    global train_y_raw\n",
    "    global test_x_raw\n",
    "    global train_x\n",
    "    global test_x\n",
    "    \n",
    "    vec = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0)\n",
    "    train_x=vec.fit_transform(train_x_raw)\n",
    "    features = vec.get_feature_names()\n",
    "    #print(len(dict(zip(features,vec.idf_))))\n",
    "    new_features = features\n",
    "    print(new_features)\n",
    "    print(features)\n",
    "    vec_less_features = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0,vocabulary=new_features)\n",
    "    train_x=vec_less_features.fit_transform(train_x_raw)\n",
    "    \n",
    "    \n",
    "    \n",
    "    vec2 = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0,vocabulary=vec_less_features.get_feature_names())\n",
    "    test_x = vec2.fit_transform(test_x_raw)\n",
    "    #print(test_x)\n",
    "\n",
    "\n",
    "    #print(\"train_x is a matrix with size : \",train_x.shape[0],train_x.shape[1])\n",
    "    #print(\"train_y is an array with size: \",len(train_y))\n",
    "\n",
    "    \n",
    "# Library function : logisticRegression    \n",
    "def logistic_regression():\n",
    "    global train_x\n",
    "    global train_y\n",
    "    global test_x\n",
    "    lr_classifier = LogisticRegression(penalty='l2', C=1)\n",
    "    lr_classifier.fit(train_x, train_y)\n",
    "    # predict on the test file\n",
    "    test_y_pred = lr_classifier.predict(test_x)\n",
    "    test_y_pred_temp = test_y_pred.tolist()\n",
    "\n",
    "    # write the output to the output file\n",
    "    with open(\"library_logistic_output.csv\",'w') as output:\n",
    "        output.write(\"Id,Category\")\n",
    "        output.write(\"\\n\")\n",
    "        for i in range(len(test_y_pred_temp)):\n",
    "            output.write(str(i))\n",
    "            output.write(\",\")\n",
    "            output.write(test_y_pred_temp[i])\n",
    "            output.write(\"\\n\")\n",
    "\n",
    "load_dataset()\n",
    "tfidf_preprocess()\n",
    "#logistic_regression()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
