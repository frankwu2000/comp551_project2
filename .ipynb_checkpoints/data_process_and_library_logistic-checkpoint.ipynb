{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start running\n",
      "start building tree...\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import csv\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "\n",
    "\n",
    "train_x_raw = []\n",
    "train_y = []\n",
    "test_x_raw = []\n",
    "\n",
    "#load two train data files, one test data file, and formatting them\n",
    "def load_dataset():\n",
    "    \n",
    "    global train_x_raw\n",
    "    global train_y_raw\n",
    "    global test_x_raw\n",
    "    \n",
    "    with open(\"data_set/train_set_x.csv\",\"r\",encoding='UTF8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader) #skip header of file\n",
    "        for row in reader:\n",
    "            text_deleteurl = re.sub(r\"http\\S+\",\"\", row[1])\n",
    "            text_deletenum=re.sub(\"\\d+\",\"\",text_deleteurl)\n",
    "            l=text_deletenum.replace(\" \",\"\").lower()\n",
    "            train_x_raw.append(l)\n",
    "\n",
    "    with open(\"data_set/train_set_y.csv\",\"r\",encoding='UTF8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader) #skip header of file\n",
    "        for row in reader:\n",
    "            train_y.append(row[1])\n",
    "        \n",
    "    with open(\"data_set/test_set_x.csv\",\"r\",encoding='UTF8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader) #skip header of file\n",
    "        for row in reader:\n",
    "            text_deletenum=re.sub(\"\\d+\",\"\",row[1])\n",
    "            l=text_deletenum.replace(\" \",\"\").lower()\n",
    "            test_x_raw.append(l)\n",
    "\n",
    "#tfidf preprocessing - convert train_x_raw and test_x_raw to sparse matrix with size of (num_documents,num_features) \n",
    "def tfidf_preprocess():\n",
    "    global train_x_raw\n",
    "    global train_y_raw\n",
    "    global test_x_raw\n",
    "    train_x = []\n",
    "    test_x = []\n",
    "    \n",
    "    vec = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0)\n",
    "    train_x=vec.fit_transform(train_x_raw)\n",
    "    features = vec.get_feature_names()\n",
    "    #print(len(dict(zip(features,vec.idf_))))\n",
    "    #new_features = features[0:220]\n",
    "    # print(new_features)\n",
    "    # print(features)\n",
    "    \n",
    "    # vec_less_features = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0,vocabulary=new_features)\n",
    "    # train_x=vec_less_features.fit_transform(train_x_raw)\n",
    "    \n",
    "    vec2 = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0,vocabulary=vec.get_feature_names())\n",
    "    test_x = vec2.fit_transform(test_x_raw)\n",
    "    # test_x = vec_less_features.fit_transform(test_x_raw)\n",
    "    #print(test_x)\n",
    "    #print(\"train_x is a matrix with size : \",train_x.shape[0],train_x.shape[1])\n",
    "    #print(\"train_y is an array with size: \",len(train_y))\n",
    "    return train_x,test_x\n",
    "    \n",
    "# Library function : logisticRegression    \n",
    "def logistic_regression(train_x,train_y,test_x):\n",
    "    lr_classifier = LogisticRegression(penalty='l2', C=1)\n",
    "    lr_classifier.fit(train_x, train_y)\n",
    "    # predict on the test file\n",
    "    test_y_pred = lr_classifier.predict(test_x)\n",
    "    test_y_pred_temp = test_y_pred.tolist()\n",
    "\n",
    "    # write the output to the output file\n",
    "    with open(\"output_data_set/library_logistic_output.csv\",'w') as output:\n",
    "        output.write(\"Id,Category\")\n",
    "        output.write(\"\\n\")\n",
    "        for i in range(len(test_y_pred_temp)):\n",
    "            output.write(str(i))\n",
    "            output.write(\",\")\n",
    "            output.write(test_y_pred_temp[i])\n",
    "            output.write(\"\\n\")\n",
    "\n",
    "\n",
    "# Decision tree node\n",
    "class Node:\n",
    "    pos_child = None\n",
    "    neg_child = None\n",
    "    split_value = None\n",
    "    is_leaf = False\n",
    "    def __init__(self,x_set,findex):\n",
    "        self.x_set = x_set\n",
    "        self.findex = findex\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Decision Tree\n",
    "class Decision_tree:\n",
    "    \n",
    "    def __init__(self,threshold,max_feature_index,train_x,train_y):\n",
    "        self.threshold = threshold\n",
    "        self.max_feature_index = max_feature_index\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        \n",
    "    def build_tree(self,cur_node):\n",
    "        if cur_node.findex > self.max_feature_index:\n",
    "            cur_node.is_leaf = True\n",
    "            #print(cur_node.x_set,cur_node.is_leaf)\n",
    "            return cur_node\n",
    "        \n",
    "        childs_set,information_gain,best_split_value = self.split_node(cur_node.findex,cur_node.x_set)\n",
    "        cur_node.split_value = best_split_value\n",
    "        if information_gain < self.threshold:\n",
    "            cur_node.is_leaf = True\n",
    "            #print(cur_node.x_set,cur_node.is_leaf)\n",
    "            return cur_node\n",
    "        #print(cur_node.x_set,cur_node.is_leaf)\n",
    "        cur_node.pos_child = Node(childs_set[0],cur_node.findex+1)\n",
    "        cur_node.neg_child = Node(childs_set[1],cur_node.findex+1)\n",
    "        self.build_tree(cur_node.pos_child)\n",
    "        self.build_tree(cur_node.neg_child)\n",
    "        return cur_node\n",
    "    \n",
    "    def predict_y(self,root,test_x):\n",
    "        predict_y = []\n",
    "        for row in range(test_x.shape[0]):\n",
    "            cur_node = root\n",
    "            for column in range(test_x.shape[1]):\n",
    "                if test_x[row,column] >= cur_node.split_value:\n",
    "                    if cur_node.pos_child:\n",
    "                        cur_node = cur_node.pos_child\n",
    "                else:\n",
    "                    if cur_node.neg_child:\n",
    "                        cur_node = cur_node.neg_child\n",
    "            predict_y.append(self.get_best_y(cur_node.x_set))\n",
    "        return predict_y\n",
    "    \n",
    "    def get_best_y(self,x_set):\n",
    "        best_len = -1\n",
    "        best_key = -1\n",
    "        for key in x_set.keys():\n",
    "            if len(x_set[key]) > best_len:\n",
    "                best_len = len(x_set[key])\n",
    "                best_key = key\n",
    "        return best_key\n",
    "    \n",
    "# help function to build tree\n",
    "    \n",
    "    #match sparse_matrix train_x and list train_y\n",
    "    #combine to be a dictionary with form:\n",
    "    #xy_set: {\n",
    "    #[class 0:[0,1,2,3,4]]\n",
    "    #[class 1:[5,6,7]]\n",
    "    #[class 2:[8,9,10]]\n",
    "    #[class 4:[11,12,13,14,15]]}\n",
    "    #where element is the documentID in train_x\n",
    "    \n",
    "    def combine_set(self):\n",
    "        xy_dic = {}\n",
    "        for i in range(len(self.train_y)):\n",
    "            if self.train_y[i] not in xy_dic:\n",
    "                xy_dic[self.train_y[i]]=[i]\n",
    "            else:\n",
    "                xy_dic[self.train_y[i]].append(i)\n",
    "        return xy_dic\n",
    "\n",
    "    #create positive children set and negative children set from a xy_set using split_value and split feature index\n",
    "    def create_child(self,parent_set,split_value,splitfeature_index):\n",
    "        pos_set = {}\n",
    "        neg_set = {}\n",
    "        for key in parent_set.keys():\n",
    "            for i in parent_set[key]:\n",
    "                if self.train_x[i,splitfeature_index] >= split_value:\n",
    "                    if key not in pos_set:\n",
    "                        pos_set[key]=[i]\n",
    "                    else:\n",
    "                        pos_set[key].append(i)\n",
    "                else:\n",
    "                    if key not in neg_set:\n",
    "                        neg_set[key]=[i]\n",
    "                    else:\n",
    "                        neg_set[key].append(i)\n",
    "        return [pos_set,neg_set]\n",
    "\n",
    "    \n",
    "    def entropy(self,xy_dic):\n",
    "        total_entropy = 0\n",
    "        total_count_set = len(xy_dic.items())\n",
    "        for key in xy_dic.keys():\n",
    "            total_entropy -= len(xy_dic[key])/total_count_set * math.log(len(xy_dic[key])/total_count_set,2)\n",
    "        return total_entropy\n",
    "\n",
    "    def information_gain(self,parent_set, split_value, splitfeature_index):\n",
    "        children = self.create_child(parent_set,split_value,splitfeature_index)\n",
    "        totalcount = len(children[0].items())+len(children[1].items())\n",
    "        return children , self.entropy(parent_set) - len(children[0].items())/totalcount*self.entropy(children[0]) - len(children[1].items())/totalcount*self.entropy(children[1])\n",
    "\n",
    "    def split_node(self,findex,parent_set):\n",
    "        split_values=[]\n",
    "        #get featured column to a list\n",
    "        featured_column = []\n",
    "        for i in range(self.train_x.shape[0]):\n",
    "            featured_column.append(self.train_x[i,findex])\n",
    "        \n",
    "        for i in range(len(featured_column)):\n",
    "            if i-1 >= 0:\n",
    "                split_values.append(featured_column[i-1]+(featured_column[i]-featured_column[i-1])/2)\n",
    "        best_info_gain = -10000\n",
    "        best_split_value = 0\n",
    "        split_value_to_children_dic = {}\n",
    "        for split_value in split_values:\n",
    "            children,temp = self.information_gain(parent_set,split_value,findex)\n",
    "            split_value_to_children_dic[temp]=children\n",
    "            if temp > best_info_gain:\n",
    "                best_info_gain = temp\n",
    "                best_split_value = split_value\n",
    "        return split_value_to_children_dic[best_info_gain],best_info_gain,best_split_value\n",
    "    \n",
    "    \n",
    "            \n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "def train_accuracy(predict_y,train_y):\n",
    "    correct=0\n",
    "    for i in range(len(predict_y)):\n",
    "        if predict_y[i] == train_y[i]:\n",
    "            correct += 1\n",
    "    return correct/len(predict_y)\n",
    "\n",
    "def output_predict_to_file(predict_y):\n",
    "    with open(\"output_data_set/decision_tree_predict.csv\",\"w\",encoding='UTF8') as output:\n",
    "        output.write(\"Id,Category\")\n",
    "        output.write(\"\\n\")\n",
    "        for i in range(len(predict_y)):\n",
    "            output.write(str(i))\n",
    "            output.write(\",\")\n",
    "            output.write(predict_y[i])\n",
    "            output.write(\"\\n\")\n",
    "            \n",
    "\n",
    "print(\"start running\")\n",
    "load_dataset()\n",
    "train_x,test_x=tfidf_preprocess()\n",
    "# logistic_regression(train_x,train_y,test_x)\n",
    "\n",
    "#decision tree\n",
    "dt = Decision_tree(0.01,train_x.shape[1]/2,train_x,train_y)\n",
    "root_set = dt.combine_set()\n",
    "root=Node(root_set,0)\n",
    "print(\"start building tree...\")\n",
    "dt.build_tree(root)\n",
    "print(\"finish building tree, start predicting y...\")\n",
    "predict_y_values=dt.predict_y(root,test_x)\n",
    "#predict_y_values=dt.predict_y(root,train_x)\n",
    "print(\"finish predicting y...\")\n",
    "output_predict_to_file(predict_y_values)\n",
    "#print(\"train accuracy: \",train_accuracy(predict_y_values,train_y))\n",
    "print(\"output file complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
