{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import csv\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import *\n",
    "\n",
    "train_x_raw = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "test_x_raw = []\n",
    "test_x = []\n",
    "\n",
    "\n",
    "#load two train data files, one test data file, and formatting them\n",
    "def load_dataset():\n",
    "    \n",
    "    global train_x_raw\n",
    "    global train_y\n",
    "    global test_x_raw\n",
    "    \n",
    "    with open(\"data_set/train_set_x.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            text_deleteurl = re.sub(r\"http\\S+\",\"\", row[1])\n",
    "            text_deletenum=re.sub(\"\\d+\",\"\",text_deleteurl)\n",
    "            l=text_deletenum.replace(\" \",\"\").lower()\n",
    "            train_x_raw.append(l)\n",
    "\n",
    "    with open(\"data_set/train_set_y.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            train_y.append(row[1])\n",
    "        \n",
    "    with open(\"data_set/test_set_x.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            text_deletenum=re.sub(\"\\d+\",\"\",row[1])\n",
    "            l=text_deletenum.replace(\" \",\"\").lower()\n",
    "            test_x_raw.append(l)\n",
    "\n",
    "def tfidf_preprocess():\n",
    "#tfidf preprocessing\n",
    "    global train_x_raw\n",
    "    global train_y_raw\n",
    "    global test_x_raw\n",
    "    global train_x\n",
    "    global test_x\n",
    "    \n",
    "    vec = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0)\n",
    "    train_x=vec.fit_transform(train_x_raw)\n",
    "    features = vec.get_feature_names()\n",
    "    #print(len(dict(zip(features,vec.idf_))))\n",
    "    #new_features = features\n",
    "    #print(new_features)\n",
    "    #print(features)\n",
    "    #vec_less_features = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0,vocabulary=new_features)\n",
    "    #train_x=vec_less_features.fit_transform(train_x_raw)\n",
    "    \n",
    "    vec2 = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0,vocabulary=vec.get_feature_names())\n",
    "    test_x = vec2.fit_transform(test_x_raw)\n",
    "    #print(test_x)\n",
    "\n",
    "\n",
    "    #print(\"train_x is a matrix with size : \",train_x.shape[0],train_x.shape[1])\n",
    "    #print(\"train_y is an array with size: \",len(train_y))\n",
    "\n",
    "    \n",
    "# Library function : logisticRegression    \n",
    "def logistic_regression():\n",
    "    global train_x\n",
    "    global train_y\n",
    "    global test_x\n",
    "    lr_classifier = LogisticRegression(penalty='l2', C=1)\n",
    "    lr_classifier.fit(train_x, train_y)\n",
    "    # predict on the test file\n",
    "    test_y_pred = lr_classifier.predict(test_x)\n",
    "    test_y_pred_temp = test_y_pred.tolist()\n",
    "\n",
    "    # write the output to the output file\n",
    "    with open(\"output_data_set/library_logistic_output.csv\",'w') as output:\n",
    "        output.write(\"Id,Category\")\n",
    "        output.write(\"\\n\")\n",
    "        for i in range(len(test_y_pred_temp)):\n",
    "            output.write(str(i))\n",
    "            output.write(\",\")\n",
    "            output.write(test_y_pred_temp[i])\n",
    "            output.write(\"\\n\")\n",
    "\n",
    "            \n",
    "# Decision Tree\n",
    "\n",
    "    \n",
    "def combine_set(train_x,train_y):\n",
    "    for i in range(len(train_y)):\n",
    "        if train_y[i] not in xy_dic:\n",
    "            xy_dic[train_y[i]]=[i]\n",
    "        else:\n",
    "            xy_dic[train_y[i]].append(i)\n",
    "    return xy_dic\n",
    "\n",
    "def split_set(parent_set,split_value,splitfeature_index):\n",
    "    pos_set = {}\n",
    "    neg_set = {}\n",
    "    for key in parent_set.keys():\n",
    "        for i in parent_set[key]:\n",
    "            if train_x[i,splitfeature_index]>=split_value:\n",
    "                if key in pos_set:\n",
    "                    pos_set[key]=[i]\n",
    "                else:\n",
    "                    pos_set[key].append(i)\n",
    "            else:\n",
    "                if key in neg_set:\n",
    "                    neg_set[key]=[i]\n",
    "                else:\n",
    "                    neg_set[key].append(i)\n",
    "    return [pos_set,neg_set]\n",
    "\n",
    "def entropy(xy_dic):\n",
    "    total_entropy = 0\n",
    "    total_count_set = 0\n",
    "    for key in xy_dic.keys():\n",
    "        total_count_set += len(xy_dic[key])\n",
    "    for key in xy_dic.keys():\n",
    "        total_entropy -= xy_dic[key]/total_count_set * math.log(xy_dic[key]/total_count_set,2)\n",
    "    return total_entropy\n",
    "\n",
    "def information_gain(parent_set, splitvalue, splitFeature_index):\n",
    "    children = split_set(parent_set,split_value,splitfeature_index)\n",
    "    totalcount = len(children[0].items())+len(children[1].items())\n",
    "    entropy(parent_set) - len(children[0].items())/totalcount*entropy(children[0]) - len(children[1].items())/totalcount*entropy(children[1])\n",
    "    \n",
    "def choose_best_split(featured_column,parent_set):\n",
    "    split_values=[]\n",
    "    for i in range(len(featured_column)):\n",
    "        if i-1 >= 0:\n",
    "            split_values.append(featured_column[i-1]+(featured_column[i]-featured_column[i-1])/2)\n",
    "    \n",
    "    best_info_gain = -10000\n",
    "    best_split_value = 0\n",
    "    for split_value in split_values:\n",
    "        temp = information_gain(parent_set,split_value,featured_column)\n",
    "        if temp > best_info_gain:\n",
    "            best_info_gain = temp\n",
    "            best_split_value = split_value\n",
    "    return best_split_value\n",
    "\n",
    "'''\n",
    "def decision_tree(parent_dic):\n",
    "    for features_index in range(train_x.shape[1]):\n",
    "        best_split_value = choose_best_split(features_index,parent_dic)\n",
    "        children = split_set(parent_dic,best_split_value,features_index)\n",
    "        if information_gain(parent_dic,best_split_value,features_index) == 0:\n",
    "            return 0\n",
    "        pos_set=decision_tree(children[0])\n",
    "        neg_set=decision_tree(children[1])\n",
    "'''\n",
    "\n",
    "\n",
    "load_dataset()\n",
    "tfidf_preprocess()\n",
    "#logistic_regression()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pseudocode for decision tree:\n",
    "\n",
    "xy_set: {\n",
    "    [class 0:[0,1,2,3,4]]\n",
    "    [class 1:[5,6,7]]\n",
    "    [class 2:[8,9,10]]\n",
    "    [class 4:[11,12,13,14,15]]\n",
    "        }\n",
    "        \n",
    "split()\n",
    "  \n",
    "def entropy(xy_set):\n",
    "    total_entropy = 0\n",
    "    for ele in xy_set:\n",
    "    for key in xy.key()\n",
    "    total_entropy -= xy_set[key]/total_count_set * math.log(xy_set[key]/total_count_set,2)\n",
    "    return total_entropy\n",
    "\n",
    "def information_gain(parent_set,split_num):\n",
    "    parent_set = entropy(parent_set)\n",
    "    return entropy(parent_set) - sum_entropy(parent_set,split)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
