{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x7f', '\\x80', '\\x92', '\\x9c', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '\\xad', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', '×', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'ÿ', 'ą', 'ć', 'č', 'ď', 'ĕ', 'ė', 'ę', 'ě', 'ğ', 'ı', 'ĺ', 'ľ', 'ł', 'ń', 'ň', 'ō', 'ő', 'œ', 'ŕ', 'ř', 'ś', 'ŝ', 'ş', 'š', 'ť', 'ū', 'ů', 'ű', 'ź', 'ż', 'ž', 'ƒ', 'ɍ', 'ɑ', 'ɚ', 'ɛ', 'ɴ', 'ʁ', 'ʒ', 'ʖ', 'ʜ']\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x7f', '\\x80', '\\x92', '\\x9c', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '\\xad', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', '×', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'ÿ', 'ą', 'ć', 'č', 'ď', 'ĕ', 'ė', 'ę', 'ě', 'ğ', 'ı', 'ĺ', 'ľ', 'ł', 'ń', 'ň', 'ō', 'ő', 'œ', 'ŕ', 'ř', 'ś', 'ŝ', 'ş', 'š', 'ť', 'ū', 'ů', 'ű', 'ź', 'ż', 'ž', 'ƒ', 'ɍ', 'ɑ', 'ɚ', 'ɛ', 'ɴ', 'ʁ', 'ʒ', 'ʖ', 'ʜ', 'ˆ', 'ˇ', 'ˈ', 'ˋ', '˜', '̀', '́', '̃', '̇', '̛', '̧', '̯', '̵', '̶', '̿', '̀', '͜', '͝', '͡', 'α', 'δ', 'ε', 'η', 'θ', 'ι', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'ω', 'ύ', 'а', 'б', 'г', 'д', 'е', 'л', 'н', 'о', 'р', 'с', 'т', 'у', 'х', 'ь', 'э', '҉', 'ԁ', 'ב', 'ד', 'ה', 'ו', 'ר', 'ת', 'ا', 'ب', 'ت', 'خ', 'د', 'ر', 'ز', 'س', 'ع', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ی', 'ಠ', 'ಥ', '༽', 'ლ', 'ᅌ', 'ᴗ', 'ᴜ', '\\u200b', '\\u200d', '–', '—', '―', '‘', '’', '‚', '“', '”', '„', '†', '‡', '•', '…', '‰', '′', '‹', '›', '‼', '‿', '⁉', '€', '℅', '™', '↑', '→', '↗', '∀', '√', '≈', '≠', '≡', '≦', '≧', '⊂', '⊄', '⌐', '⌘', '⌚', '⎘', '⏩', '⏱', 'ⓞ', '─', '┊', '┬', '╭', '╲', '▀', '■', '▶', '▷', '▼', '◁', '●', '◕', '◡', '☀', '★', '☒', '☕', '☞', '☹', '☺', '♀', '♂', '♛', '♡', '♥', '♦', '♪', '♫', '⚆', '⚠', '⚡', '⚪', '⚽', '⛳', '✅', '✈', '✊', '✌', '✓', '✔', '✥', '✨', '✿', '❤', '❶', '➍', '➎', '➏', '➐', '➡', '⭐', '「', '」', '【', '】', 'え', 'き', 'し', 'す', 'つ', 'ま', 'み', 'ツ', 'ノ', 'モ', 'ヮ', 'ヽ', 'ヾ', '乇', '卐', '天', '始', '开', '我', '明', '点', '益', '秀', '\\uf0d8', '\\uf8ff', '️', '﹏', '\\ufeff', '！', '＇', '）', '，', '？', '｀', 'ａ', 'ｃ', 'ｄ', 'ｅ', 'ｇ', 'ｈ', 'ｉ', 'ｌ', 'ｎ', 'ｏ', 'ｐ', 'ｒ', 'ｓ', 'ｕ', 'ﾉ', 'ﾟ', '�', '𝄞', '𝐀', '𝐄', '𝐑', '𝐙', '𝔐', '𝔞', '𝔢', '𝔦', '𝔫', '𝔯', '𝘓', '𝘢', '𝘤', '𝘥', '𝘦', '𝘧', '𝘩', '𝘪', '𝘭', '𝘳', '𝘴', '𝘵', '𝘶', '𝘹', '🅱', '🇦', '🇧', '🇪', '🇫', '🇬', '🇮', '🇷', '🇸', '🇺', '🌀', '🌈', '🌍', '🌜', '🌞', '🌟', '🌠', '🌳', '🌶', '🌷', '🌹', '🌻', '🌼', '🌿', '🍃', '🍫', '🍷', '🍻', '🍿', '🎁', '🎂', '🎉', '🎊', '🎤', '🎥', '🎧', '🎬', '🎮', '🎵', '🎶', '🎺', '🏀', '🏁', '🏃', '🏆', '🏋', '🏌', '🏍', '🏎', '🏫', '🏴', '🏻', '🏼', '🏽', '🏾', '🏿', '🐐', '🐒', '👀', '👅', '👆', '👉', '👊', '👋', '👌', '👍', '👎', '👏', '👐', '👔', '👥', '👨', '👩', '👮', '👲', '💃', '💅', '💋', '💐', '💓', '💔', '💕', '💖', '💙', '💚', '💛', '💜', '💤', '💥', '💩', '💪', '💫', '💬', '💯', '💻', '📈', '📍', '📐', '📚', '📢', '📲', '📺', '📽', '🔝', '🔥', '🔴', '🔵', '🕊', '🖒', '🖱', '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😈', '😉', '😊', '😋', '😌', '😍', '😎', '😏', '😐', '😑', '😒', '😓', '😔', '😕', '😖', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😠', '😡', '😢', '😣', '😤', '😥', '😧', '😨', '😩', '😪', '😫', '😬', '😭', '😮', '😯', '😱', '😲', '😳', '😴', '😵', '😶', '😷', '😹', '😻', '🙁', '🙂', '🙃', '🙄', '🙅', '🙇', '🙈', '🙊', '🙋', '🙌', '🙍', '🙏', '🚅', '🚋', '🚑', '🚕', '🚨', '🚫', '🚶', '🚽', '🚿', '🤒', '🤓', '🤔', '🤗', '🤘', '🤙', '🤞', '🤠', '🤡', '🤢', '🤣', '🤤', '🤥', '🤦', '\\U0001f929', '\\U0001f92a', '🤷', '🥀', '🥇', '🥙', '🦄', '🦊', '🦋']\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import csv\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import *\n",
    "\n",
    "train_x_raw = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "test_x_raw = []\n",
    "test_x = []\n",
    "\n",
    "\n",
    "#load two train data files, one test data file, and formatting them\n",
    "def load_dataset():\n",
    "    \n",
    "    global train_x_raw\n",
    "    global train_y\n",
    "    global test_x_raw\n",
    "    \n",
    "    with open(\"data_set/train_set_x.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            text_deleteurl = re.sub(r\"http\\S+\",\"\", row[1])\n",
    "            text_deletenum=re.sub(\"\\d+\",\"\",text_deleteurl)\n",
    "            l=text_deletenum.replace(\" \",\"\").lower()\n",
    "            train_x_raw.append(l)\n",
    "\n",
    "    with open(\"data_set/train_set_y.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            train_y.append(row[1])\n",
    "        \n",
    "    with open(\"data_set/test_set_x.csv\",\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader,None) #skip header of file\n",
    "        for row in reader:\n",
    "            text_deletenum=re.sub(\"\\d+\",\"\",row[1])\n",
    "            l=text_deletenum.replace(\" \",\"\").lower()\n",
    "            test_x_raw.append(l)\n",
    "\n",
    "def tfidf_preprocess():\n",
    "#tfidf preprocessing\n",
    "    global train_x_raw\n",
    "    global train_y_raw\n",
    "    global test_x_raw\n",
    "    global train_x\n",
    "    global test_x\n",
    "    \n",
    "    vec = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0)\n",
    "    train_x=vec.fit_transform(train_x_raw)\n",
    "    features = vec.get_feature_names()\n",
    "    #print(len(dict(zip(features,vec.idf_))))\n",
    "    new_features = features\n",
    "    print(new_features)\n",
    "    print(features)\n",
    "    vec_less_features = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0,vocabulary=new_features)\n",
    "    train_x=vec_less_features.fit_transform(train_x_raw)\n",
    "    \n",
    "    \n",
    "    \n",
    "    vec2 = TfidfVectorizer(decode_error='strict',analyzer='char',min_df=0,vocabulary=vec_less_features.get_feature_names())\n",
    "    test_x = vec2.fit_transform(test_x_raw)\n",
    "    #print(test_x)\n",
    "\n",
    "\n",
    "    #print(\"train_x is a matrix with size : \",train_x.shape[0],train_x.shape[1])\n",
    "    #print(\"train_y is an array with size: \",len(train_y))\n",
    "\n",
    "    \n",
    "# Library function : logisticRegression    \n",
    "def logistic_regression():\n",
    "    global train_x\n",
    "    global train_y\n",
    "    global test_x\n",
    "    lr_classifier = LogisticRegression(penalty='l2', C=1)\n",
    "    lr_classifier.fit(train_x, train_y)\n",
    "    # predict on the test file\n",
    "    test_y_pred = lr_classifier.predict(test_x)\n",
    "    test_y_pred_temp = test_y_pred.tolist()\n",
    "\n",
    "    # write the output to the output file\n",
    "    with open(\"library_logistic_output.csv\",'w') as output:\n",
    "        output.write(\"Id,Category\")\n",
    "        output.write(\"\\n\")\n",
    "        for i in range(len(test_y_pred_temp)):\n",
    "            output.write(str(i))\n",
    "            output.write(\",\")\n",
    "            output.write(test_y_pred_temp[i])\n",
    "            output.write(\"\\n\")\n",
    "\n",
    "load_dataset()\n",
    "tfidf_preprocess()\n",
    "#logistic_regression()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
